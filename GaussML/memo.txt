「ガウス過程の機械学習」のノートと誤植訂正(3章)
メモ　機械学習　ガウス過程

$3$章からガウス過程に入り、かなり難しくなる印象です。数式も強そうな見た目になってきますが、一つ一つ丁寧に追いかけてみるとそこまで難しいことはしていません。ガウス過程の部分については分かりやすく描かれていますが、「次元」という言葉に惑わされてしまいました。

#3.1 線形回帰モデルと次元の呪い
この章の初めの方では「次元」という言葉が二つの意味で用いられているので、ここでそれを明確に区別しておこうと思います。

空間の次元…$1$つのデータ$\boldsymbol{x}$の成分数のことです。もう少し厳密に書くと、$\boldsymbol{x} \in R^n$の$n$のことです。
データ数の次元…単にデータ数のことです。後で出てくる「無限次元のガウス分布」の無限次元はこちらのことを指します。

この節では線形回帰モデルの基底関数として動径基底関数
$$
\phi_h (x) = \exp \left( - \dfrac{ (\boldsymbol{x} - \boldsymbol{\mu}_h )^2} { \sigma^2} \right)
$$
を選ぶことを考えています。しかしこのとき空間の次元が大きいとき(つまり$\boldsymbol{x} \in R^n$として$n >> 1$のとき)、必要なパラメータ数は刻み幅の数$2H$を用いて$(2H)^n$のように増加してしまいます。これを指して次元の呪いと言っていますが，厳密にはここでの次元は空間の次元ですね。本書でも次元に対して指数的にパラメータが増加すると述べられています。

#3.2　ガウス過程
テキストでは厳密に線形回帰できるときを考えていますが、誤差を考慮に入れても同じように考えることができます。というか後の方を見るとそれについても扱っていますが、同じ形で導出した方がわかりやすいと思うので一応示しておきます。誤差は平均$0$で分散$\sigma$の正規分布に従うものとします。
```math
\begin{eqnarray}
\langle \boldsymbol{y} \rangle &=& \langle \Phi \boldsymbol{w} + \boldsymbol{\epsilon} \rangle  0 \\
\Sigma &=& \langle \boldsymbol{y} \boldsymbol{y} ^T \rangle -\langle \boldsymbol{y}\rangle \langle \boldsymbol{y} ^T \rangle 
 &=& \Phi \langle \boldsymbol{w} \boldsymbol{w}^T \rangle \Phi ^T + \Phi \langle \boldsymbol{w} \boldsymbol{\epsilon}^T \rangle
+ \langle \boldsymbol{\epsilon} \boldsymbol{w}^T \rangle \Phi^T
+ \langle \boldsymbol{\epsilon} \boldsymbol{\epsilon}^T \langle
 = \lambda \Phi \Phi^T + \sigma^2
```
縦ベクトル$\times$横ベクトルは行列になることに注意します。また$ \langle \boldsymbol{w} \boldsymbol{\epsilon}^T \rangle = 0$が成立するのは、これらの確率変数が独立ということを利用しています(詳しく言うと、独立な確率変数の積の期待値は期待値の積になり、各々の期待値は0だからです)。
結局のところ、共分散行列の対角成分に余計な項が加わっただけという感じです。つまりあまり気にする必要はなかったということです。

ここで注意しなければならないポイントは$2$つあります。一つ目は$\boldsymbol{w}$が消えてしまったということです。これは$\boldsymbol{w}$に確率性を導入して、期待値をとることで消してしまったということですね。パラメータがかなり減ってうれしいです。
二つ目はここで出てきた「無限次元のガウス分布」についてです。前述の通り、これはデータ数の次元を無限にしたものだということです。(データ数を無限にするというわけではない…？)

#3.2.1　ガウス過程の意味
与えられた$2$つのデータ点があったときに、これらの特徴を表す特徴量空間への写像$\phi$を考えて、特徴量空間での$2$つの"近さ"というものを測ります。それが近いとき、相関が強くなり似たような出力をするということです。
ここを読むと、確かに自然言語処理などと相性がいいのかもしれないと感じました。言語のようなそのままでは扱いにくものを特徴量空間に飛ばして扱うという手法が面白いです。

#3.2.2カーネルトリック
